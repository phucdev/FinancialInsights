# FinancialInsights

A real-time financial news sentiment analysis and stock trend correlation engine showcasing modern data engineering and ML engineering practices.

## Overview

The goal is to build a streaming data pipeline that ingests real-time financial news articles and stock price data, performs sentiment analysis on the news articles, and correlates the sentiment with stock price movements. The processed data is then stored in a MySQL database and made accessible via a FastAPI web service for visualization and further analysis.
In addition I want to use Toto-Open-Base-1.0 for time series forecasting of stock prices based on historical data and sentiment trends.

This is a work in progress project for learning and demonstrating skills in data engineering, machine learning engineering, and modern data stack technologies.

## Tech Stack

- **Streaming**: Apache Kafka for real-time data ingestion
- **Database**: MySQL for historical data storage
- **API**: FastAPI for data access and visualization
- **Orchestration**: Docker Compose for containerized deployment

## Project Structure
```
FinancialInsights/ 
â”œâ”€â”€ config/ # Configuration files 
â”œâ”€â”€ data/examples/ # Sample datasets (news.csv, prices.csv) 
â”œâ”€â”€ db/ # Database initialization scripts 
â”œâ”€â”€ kafka/ # Kafka topic setup scripts 
â”œâ”€â”€ pipelines/ # Data pipeline components 
â”‚ â”œâ”€â”€ producers/ # Kafka producers for news and prices 
â”‚ â””â”€â”€ ingestor_consumer.py # Kafka consumer 
â”œâ”€â”€ services/api/ # FastAPI service 
â””â”€â”€ docker-compose.yml # Container orchestration
```

## Getting Started

### Prerequisites

- Docker
- Python 3.12
- Make (optional, for using Makefile commands)
- [uv for dependency management](https://docs.astral.sh/uv/getting-started/installation/)

1. Clone the repository:
    ```bash
    git clone https://github.com/phucdev/FinancialInsights.git
    cd FinancialInsights
    ```
2. To install the dependencies, either use `make install` or do it manually with `uv`:
   ```bash
   uv venv && . .venv/bin/activate && uv sync --frozen
   ```
3. Copy the .env.example to .env and adjust configurations as needed:
   ```bash
   cp .env.example .env
   ```
4. Start the services using Docker Compose:
   ```bash
   docker-compose up -d
   ```
   In addition to starting Kafka, MySQL, the API server this will also run a helper that creates the news and prices topics.
5. Run the data producers to start sending synthetic data to Kafka:
   ```bash
   python pipelines/producers/news_producer.py
   ```
   And in another terminal run the consumer:
   ```bash
   python pipelines/ingestor_consumer.py
   ```

## Progress

### Phase 1 (Completed)
- âœ… Project structure setup
- âœ… Kafka broker configuration
- âœ… News producers with synthetically generated data
- âœ… Consumer for data ingestion

### Phase 2 (In Progress)
- ğŸ”„ Producer/consumer refinement
- ğŸ”„ MySQL integration for historical data storage
- ğŸ”„ Data processing and transformation pipelines

### Phase 3 (Planned)
- â³ Sentiment analysis implementation
- â³ Stock trend correlation engine
- â³ Real-time analytics dashboard
- â³ API endpoints for data access

Contact
GitHub: [@phucdev](https://github.com/phucdev)